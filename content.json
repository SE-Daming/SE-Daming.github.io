[{"title":"慢SQL可能的原因","date":"2024-10-23T13:30:19.000Z","path":"2024/10/23/慢SQL可能的原因/","text":"1 未正确使用索引； 复杂查询，使用多个连接，子查询 表数据量太大； 锁竞争； 硬件资源不足、cpu、io、内存； 根据非索引字段排序造成filesort 深度分页查询； 查询不必要的列；","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://se-daming.github.io/tags/MySQL/"}]},{"title":"薄弱点","date":"2024-10-23T13:14:16.000Z","path":"2024/10/23/薄弱点/","text":"1 慢SQL、介绍实习时引出慢SQL，将下如何定位排查、如何解决 现在有user表，它的数据量很大，做了分表处理，现在有a和b两个字段，如何做到使用a或者b都可以查到某条数据在哪张表中 如何创建正确高效的索引 redis哈希表渐进式哈希kafka如何保证rebalance的时候消息不丢失","tags":[{"name":"百度首面","slug":"百度首面","permalink":"https://se-daming.github.io/tags/%E7%99%BE%E5%BA%A6%E9%A6%96%E9%9D%A2/"}]},{"title":"小红书","date":"2024-10-22T07:51:42.000Z","path":"2024/10/22/小红书/","text":"项目项目是一个仿照小红书的UGC平台，用户可以发布文章，管理发文草稿、查询历史发文、查询浏览记录、搜索记录、评论、点赞、收藏、滚动查询、私聊、用户标签、关注、粉丝、AI问答 文章如何让其他用户看到 发文模块 订单模块 私聊模块 购物模块 登录模块 用户模块 微服务模块用户模块登录模块搜索模块笔记、用户 评论模块给评论点赞、先操作redis、再xxl-job定时更新到数据库 私聊模块笔记模块增删改查、设为未公开、收藏点赞 发文章发文幂等性Redis+AOP实现自定义注解Idempotent 流程通过经纬度获取省份、保存笔记到数据库，笔记id更新到布隆过滤器、通过mq将笔记异步保存到es、处理@人通知，通过mq发送、 redis的数据结构？ 有关API与小红书的不同，有哪些实现想法","tags":[{"name":"项目","slug":"项目","permalink":"https://se-daming.github.io/tags/%E9%A1%B9%E7%9B%AE/"}]},{"title":"Redis集群","date":"2024-10-20T08:33:41.000Z","path":"2024/10/20/Redis集群/","text":"主从集群主从同步全量同步从服务器第一次连接到主服务器时； 从服务器数据丢失时，会请求全量同步； 从服务器与主服务器的数据差异太大； 同步过程从服务器发送SYNC命令，请求同步； 主服务器收到后生成RDB快照发送给从服务器 ； 从服务器收到后清空当前数据集载入RDB数据； 在RDB生成和传输期间，主服务器把收到的命令写到replication backlog buffer (复制备份缓冲区) RBD传输完成后将这个buffer发送给从服务器执行 如何知道是否是第一次主从同步时从服务器会发送给主服务器replication id和offset。 replication id：数据集的标记，每一个maser有一个唯一的 offset：偏移量 master判断从节点是否是第一次就是看replication id是否一致 增量同步slave只更新与master不同的部分 slave如何知道与master的差异呢与raplication-backlog有关、这个文件是固定大小的环形数组，会记录redis命令和offset、包括master当前的offset、slave同步到的offset 依据offset、它记录了主从之间在replication-backlog的差异。 缓冲区尽量大一些，防止频繁全量同步 哨兵集群作用监控、选主、通知 监控主观下线一个sentinel检测到某个节点未在及时响应ping，则认为主观下线 客观下线超过quonum数量的sentinel都认为某节点主观下线，则该节点客观下线 选主某redis节点经过主观下线、客观下线后要选举出新的master 要先选举出sentinel中的leader。 当一个sentinel节点确认redis节点主观下线后，请求其他sentinel节点将自己选举为leader，被请求的sentinel只能同意一个请求 当选举票数达到最低票数后该sentinel为leader，由它来选择一个redis节点作为master，否则重新选举 流程为： 过滤故障节点、选择优先级slave-priority最高的节点，不存在则选择复制偏移量最大的节点，不存在则选择runid最小的节点作为新的master 通知sentinel给被选择的slave节点发送slaveof No one命令，让其成为master sentinel给其他slave发送slaveof命令，使他们成为新master的从节点 sentinel把故障节点标记为slave，当其恢复后成为新的slave节点 cluster集群一个切片集群中有16384个哈希槽，会经平均分配或手动分配映射到具体的节点 当操作数据时先计算他的哈希槽的索引确定在哪台节点然后去操作 如何计算哈希槽的索引如果没有”{}“，则根据key来计算，如果有”{}“，根据里面的内容计算","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"Redis的底层数据结构是怎样的","date":"2024-10-20T07:26:25.000Z","path":"2024/10/20/Redis的底层数据结构是怎样的/","text":"s 数据类型和底层结构关系 SDS为什么不直接使用c语言的获取字符串长度要运算； 不可修改； 非二进制安全（以空字符\\0 结尾，若数据中包含可能导致中断） 结构 len:字符串长度 alloc：分配字节数； flags：header类型 buf：数据 内存预分配假如我们要给SDS追加一段字符串“,Amy”，这里首先会申请新内存空间： 如果新字符串小于1M，则新空间为扩展后字符串长度的两倍+1； 如果新字符串大于1M，则新空间为扩展后字符串长度+1M+1。称为内存预分配。 优点支持动态扩容； 获取字符串长度O1； 减少内存分配次数； 二进制安全 压缩列表可看成是连续内存的双向链表，链表通过记录上一节点和本节点长度来寻址。内存占用较低 结构 bytes:总字节数 len:节点数 tail:尾偏移量，表位节点到起始的距离 entry:存储数据的节点 end:结束标识 Entry结构 previous_entry_length:前一个节点长度、占用1或5个字节。前一个长度小于254字节，则占用1字节 encoding：记录content的数据类型（字符串还是整数），1、2、5个字节（整数固定1字节，字符串则和字符串长度有关） content：节点数据 连锁更新问题一开始N个连续的250-253字节节点，插入了一个255字节节点。导致previous-entry-length变成5个节点，后面节点也跟着改变 跳表是多层的有序链表、每层都是一条有序双向链表、每个节点包含多层指针、底层包含了所有元素、平均Ologn、最坏On 如何设置层高结构层级、长度、首尾节点 节点：权重、数据、前一个元素的指针、多级索引数组 多级索引：下一个节点、索引跨度 为什么不用B+树、红黑树","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"Redis过期删除和缓存淘汰","date":"2024-10-20T07:05:01.000Z","path":"2024/10/20/Redis过期删除和缓存淘汰/","text":"内存淘汰分类八种两类 不进行内存淘汰，noeviction，有新的数据写入会禁止写入 进行内存淘汰，又可分为在设置了ttl中淘汰和在所有数据中淘汰 设置了TTL：随机淘汰、淘汰ttl早的、lru（淘汰设置了过期时间中最久未使用的）、lfu（淘汰最少使用的） 所有数据：随即淘汰、LRU、LFU 过期删除采用惰性删除和定期删除两种策略搭配使用，在CPU和内存浪费之间平衡 惰性删除在访问key时看是否过期。过期则删除 定期删除每隔一段时间从redis取出一定数据检查，删除其中过期的 隔多久默认是hz10 每秒10次 取多少默认是选择20个， 检查这20个是否过期，删除过期的 如果过期的大于25%，则再选20个重复，直到比例小于25%","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"Redis持久化如何实现","date":"2024-10-20T06:18:30.000Z","path":"2024/10/20/Redis持久化如何实现/","text":"Redis持久化方式RDB、AOF、RDB+AOF混合持久化 RDB原理redis在某个时间点对内存里的数据创建的全量快照 AOF原理每执行一条redis命令会将该命令写道AOF缓冲区，然后再写到AOF文件里 为什么执行完命令之后写日志？不用进行额外的语法检查； 不会阻塞当前命令执行； 但是会阻塞下一个命令执行；执行完redis命令还没写日志则数据丢失 AOF流程 append：所有的写命令追加到AOF缓冲区 write：将AOF缓冲区数据写到AOF文件，但此时数据并没有写到硬盘，而是写到了系统内核缓冲区，等待内核写入硬盘 fsync：将AOF缓冲区的数据写到硬盘，这一步阻塞直到写入完成 rewrite：随着AOF文件越来越大会重写 reload：Redis重启时可以加载AOF文件进行数据恢复 三种fsync策略always：每次调用write后都会立即调用fsync写入到硬盘 everysec：执行write后立即返回，每隔一秒后台线程调用fsync no：执行write后立即返回，由os决定如何调用，一般Linux为30s&#x2F;次 AOF重写AOF变大时会有重写机制，后台自动重写AOF产生新的AOF文件，新的文件和原文件数据一致但占用更小 redis会将AOF重写放入到子线程执行，重写期间会维护一个重写缓冲区，该缓冲区在子线程创建新AOF期间记录所有的写命令，当新AOF创建完成后，会将重写缓冲区的数据追加到新AOF末尾。最后用新AOF代替旧AOF 为什么不用现有的AOF？如果AOF重写失败，防止污染现有的文件 混合持久化混合持久化表现在AOF重写过程 当重写时会将数据以RDB的格式写入到AOF，重写缓冲区的数据是AOF格式。 也就是说、AOF前半部分是RDB格式的全量数据，后半部分是AOF的增量数据 RDB对比AOFRDB更适合恢复数据，恢复数据时直接还原即可，不需要一条一条执行命令，速度快 RDB存储的是压缩的二进制数据，文件很小。AOF存储的是写命令，文件占用大，RDB更适合做数据备份 AOF的安全更高，可以实时或秒级别的持久化；AOF更加轻量，是追加写命令道AOF文件，RDB是对数据做全量快照，生成过程繁重 AOF更容易理解解析 如何选用Redis 保存的数据丢失一些也没什么影响的话，可以选择使用 RDB。 不建议单独使用 AOF，因为时不时地创建一个 RDB 快照可以进行数据库备份、更快的重启以及解决 AOF 引擎错误。 如果保存的数据要求安全性比较高的话，建议同时开启 RDB 和 AOF 持久化或者开启 RDB 和 AOF 混合持久化","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"大key、热key是什么","date":"2024-10-20T02:47:32.000Z","path":"2024/10/20/大key、热key是什么/","text":"q 大key是什么key对应value内存占用大的 会怎样如何解决将一个key拆分为多个key","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"CMS和G1的区别","date":"2024-10-19T13:02:45.000Z","path":"2024/10/19/CMS和G1的区别/","text":"使用范围不一样CMS是老年代的垃圾回收器，配合新生代的Serial和ParNew一块使用； G1的收集范围是新生代和老年代，不需要其他垃圾回收器 STW的时间方面CMS的目标是最小停顿时间； G1可预测垃圾回收的停顿时间 垃圾回收算法不同CMS是标记清除，会产生内存碎片； G1是标记整理，不会产生内存碎片 垃圾回收的过程不同CMS：初始标记&#x3D;》并发标记&#x3D;》重新标记&#x3D;》并发清除 G1： 初始标记&#x3D;》并发标记&#x3D;》最终标记&#x3D;》筛选回收 主要表现在第四阶段：CMS用户线程和垃圾回收线程同时进行，G1用户线程暂停 STW的阶段： CMS：初始标记、重新标记 G1：初始标记、最终标记 CMS会产生浮动垃圾第四阶段是并发清除的，垃圾回收线程和用户线程同时进行会产生浮动垃圾，CMS要预留一部分用于存放浮动垃圾。当浮动垃圾过多时CMS会退化为serial-old单线程回收器，效率低； G1没有浮动垃圾，第四阶段用户线程暂停 适用场景不同CMS：高吞吐量、低延迟需求，对停顿时间敏感；老年代收集；碎片化管理； G1：适用于需要管理大堆内存的场景；对内存碎片明暗；比较平衡的性能，提供较低停顿时间的同时保持了较高的吞吐量","tags":[{"name":"JVM","slug":"JVM","permalink":"https://se-daming.github.io/tags/JVM/"}]},{"title":"RR级别真的解决幻读了吗","date":"2024-10-19T12:40:08.000Z","path":"2024/10/19/RR级别真的解决幻读了吗/","text":"m 解决了部分幻读 如果查询时用到了快照读则还是能读到其他事务操作的数据。因为它的原理就是利用MVCC下的当前读","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://se-daming.github.io/tags/MySQL/"}]},{"title":"Kafka中消费者组是什么","date":"2024-10-19T08:21:44.000Z","path":"2024/10/19/Kafka中消费者组是什么/","text":"消费者组具有相同group-id的一组消费者，为了配合分区而存在的。 分区和消费者组中的消费者是多对一的关系 消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。 消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://se-daming.github.io/tags/Kafka/"}]},{"title":"Kafka如何调优？","date":"2024-10-19T08:00:04.000Z","path":"2024/10/19/Kafka如何调优？/","text":"","tags":[]},{"title":"Kafka中消费者事务是什么？","date":"2024-10-19T07:59:48.000Z","path":"2024/10/19/Kafka中消费者事务是什么？/","text":"","tags":[]},{"title":"Kafka中leader的选举流程是怎样的","date":"2024-10-19T07:59:34.000Z","path":"2024/10/19/Kafka中leader的选举流程是怎样的/","text":"","tags":[]},{"title":"Kafka消息挤压了，怎么办","date":"2024-10-19T07:58:59.000Z","path":"2024/10/19/Kafka消息挤压了，怎么办/","text":"消息积压的原因生产快消费慢解决方案1、增加主题的分区数，同时增加消费者数，扩展消费者组 2、提高消费者每次从服务端拉取的字节数fetch.max.bytes（默认是50M）、每次拉取的最大条数max.poll.records。（达到这俩参数的其中一个就会拉取停止） 3、优化消费者处理逻辑 4、生产者限流 5、持续监控Kafka集群，比如用eagle","tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://se-daming.github.io/tags/Kafka/"}]},{"title":"Kafka消息重复","date":"2024-10-19T07:53:50.000Z","path":"2024/10/19/Kafka消息重复/","text":"生产者重复acks设为-1时，leader收到消息后ISR同步了消息，此时leader还未返回ack宕机了。生产者认为broker没有收到消息会重复发送消息给broker中的新选举的leader 消费者重复设置手动提交offset时可能出现重复消费","tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://se-daming.github.io/tags/Kafka/"}]},{"title":"Kafka如何保证消息可靠性","date":"2024-10-19T07:40:20.000Z","path":"2024/10/19/Kafka如何保证消息可靠性/","text":"broker多副本机制每个分区可以有多个副本，这些副本分为一个leader和多个follower，当leader故障时，会从follower中选举出新的leader，增加了数据可用性 确认机制可以设置三种不同的确认级别acks acks&#x3D;0：消息发送到分区时不需要等待确认akcs&#x3D;1：消息发送到分区后只需leader确认acks&#x3D;ALL：消息发送到分区后leader和follower都需要确认消息持久性Kafka会将消息持久化到磁盘，其中每一个分区对应于一个磁盘文件，确保系统故障或重启时不会丢失数据 生产者发送消息时，selector从NetworkClient中取出发送请求后发送给broker，发送成功则等待broker的应答acks，发送失败会重试 消费者消费者消费时可以设置手动提交offset，确保消息不会丢失","tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://se-daming.github.io/tags/Kafka/"}]},{"title":"Kafka如何保证消息顺序性","date":"2024-10-19T07:14:37.000Z","path":"2024/10/19/Kafka如何保证消息顺序性/","text":"为什么保证顺序性创建订单，订单支付是两个顺序的操作 如何保证顺序性生产顺序性在同一个分区内，消息是追加写到文件末尾的，可以保证消息写的顺序性 如果希望某一类消息有序，通过设置指定key&#x2F;partition使消息发送到同一个partition中，然后指定一个消费者来消费,保证生产的顺序性 消费顺序性每次消费时都会从上次的offset处消费，能够保证消费的顺序性； 每一个分区只能由一个消费者消费，保证顺序性 消息发送重试时发送顺序如果创建订单发送失败，订单支付发送成功，这样也会导致顺序性失效 1: Kafka中有个max.in.flight.requests.per.connection的参数,设为1，它告诉生产者在收到服务端响应之前只能发送一条消息，但会降低吞吐量 2: enable.idempotence&#x3D;true 开启生产者幂等性，原理与producer id和Sequence Number两个参数有关","tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://se-daming.github.io/tags/Kafka/"}]},{"title":"索引下推（ICP）是什么","date":"2024-10-18T14:22:22.000Z","path":"2024/10/18/索引下推是什么/","text":"索引下推是什么索引优化功能，允许存储引擎在索引遍历过程中执行where的条件，直接 过滤掉不满足条件的结果从而减少回表次数，提高查询效率。 它就是为了减少回表次数的 索引下推原理select * from stu where name&#x3D;’zhangsan’ and age&#x3D;18 （name，age是联合索引） MySQL架构可简单分为server层和存储引擎层， 当没有索引下推时，存储引擎层查询到name是张三的id后根据这些id进行回表，需要回表多次，然后把全部的记录返回给server进行筛选 有索引下推时，存储引擎层查询到name是张三的id并且去过滤掉age是18的，然后回表后把数据返回server 没有ICP 有ICP","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://se-daming.github.io/tags/MySQL/"}]},{"title":"Redis渐进式哈希","date":"2024-10-18T13:37:06.000Z","path":"2024/10/18/Redis渐进式哈希/","text":"Redis底层是使用dict存储数据的，包含主、备用两个哈希表，当数据量达到一定时会触发rehash重新调整，每次处理请求时会把主哈希表的数据迁移部分到备用哈希表，这叫Rehash。 因为一次rehash会导致短暂的服务中断，所以分多次来实现。叫做渐进式哈希 Rahash涉及到ReHashIdx，初始值为-1，每次处理请求会加1并把主哈希表对应位置的元素迁移到备用哈希表。 在此过程中，如果操作元素key大于ReHashIdx则访问旧哈希表，否则访问新哈希表","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"快手","date":"2024-10-18T10:58:35.000Z","path":"2024/10/18/快手-1/","text":"mysql 三大日志redolog：保证数据持久性 undolog：事务异常的回退、MVCC的版本链 binlog:做数据备份和主从同步 其中binlog的底层数据结构说一下分别细说三大日志的作用幻读概念mysql的隔离级别mysql inodb的数据结构B+树和B树有啥区别（为啥比B树快）索引下推知道吗voliate关键字有什么作用详细说一下JMM内存模型voliate关键字为啥不能保证原子性","tags":[{"name":"快手日常","slug":"快手日常","permalink":"https://se-daming.github.io/tags/%E5%BF%AB%E6%89%8B%E6%97%A5%E5%B8%B8/"}]},{"title":"JVM内存区域","date":"2024-10-18T09:43:19.000Z","path":"2024/10/18/JVM内存区域/","text":"","tags":[]},{"title":"JMM到底是什么","date":"2024-10-18T08:02:35.000Z","path":"2024/10/18/JMM到底是什么/","text":"JMM是什么JMM是Java定义的内存模型，定义了Java程序中线程如何与内存进行交互的规则，简化多线程编程，确保多线程环境下数据一致性和可见性 从开发人员：JMM是一组规范、保证了指令重排时的字段可见性 从JVM角度：因为不同的os内存访问有一定差异，所以可能造成相同代码运行在不同的系统上可能有问题，JMM屏蔽掉了这种差异，使得Java程序在不同os下运行达到同样的并发效果 JMM规定：所有变量存储在主内存中，包括实例变量，静态变量，不包括局部变量和方法参数。每个线程有自己的工作内存，工作内存保存了线程用到的变量和主内存的拷贝副本，线程对变量的操作在工作内存中进行，不能直接读写主内存的变量 并发编程三个特点原子性通过sync关键字或加锁来实现 可见性通过volatile、synchronized、lock以及原子类来实现 有序性通过sync、volatile或其他同步工具 happens-before什么是hbhb是用来描述和可见性相关的问题的，如果第一个操作hb第二个操作，那么第一个操作的结果对第二个操作是可见的。 也就是说、hb是用来表达操作可见性的 hb的规则 1、在同一线程内，前面的hp后面的操作 2、解锁操作hb加锁操作 3、对volatile的写hb对它的读，也就是说，volatile修饰的变量修改后其他线程一定能读到最新值 4、线程的start操作hb run方法的每一个操作 5、中断规则、传递规则、并发工具类的规则","tags":[{"name":"JUC","slug":"JUC","permalink":"https://se-daming.github.io/tags/JUC/"}]},{"title":"Java优先队列","date":"2024-10-18T04:44:59.000Z","path":"2024/10/18/Java优先队列/","text":"PriorityQueue123456789101112131415161718192021pq.poll();pq.offer();//队列满返回falsepq.add();//队列满抛异常PriorityQueue&lt;String&gt; pq = new PriorityQueue&lt;&gt;(new Comparator&lt;String&gt;() &#123; @Override public int compare(String s1, String s2) &#123; return Integer.compare(s1.length(), s2.length()); // 按长度升序 &#125;&#125;);// 添加元素pq.add(&quot;ale&quot;);pq.add(&quot;banana&quot;);pq.add(&quot;kiwi&quot;);pq.add(&quot;grape&quot;);// 打印并移除元素，按长度优先级while (!pq.isEmpty()) &#123; System.out.println(pq.poll()); // 输出 ale,kiwi, grape,banana&#125;","tags":[{"name":"集合","slug":"集合","permalink":"https://se-daming.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"volatile浅析","date":"2024-10-18T03:58:24.000Z","path":"2024/10/18/volatile浅析/","text":"volatile作用保证可见性、禁止指令重排 可见性原理指示JVM使用这个变量时去主存中获取 具体些说：每一个线程都有自己的本地内存，线程间共享主内存。 volatile内存可见性主要通过lock前缀指令实现，它会锁定当前内存区域的缓存，并且立即将当前缓存行写入到主内存，会写主内存的时候通过MESI协议使其他线程缓存的改变量失效，导致其他线程也要去主内存去重新读取 有序性原理插入特定的内存屏障禁止指令重排序 volatile 的有序性是通过插入内存屏障（Memory Barrier），在内存屏障前后禁止重排序优化，以此实现有序性的。 什么是内存屏障？内存屏障（Memory Barrier 或 Memory Fence）是一种硬件级别的同步操作，它强制处理器按照特定顺序执行内存访问操作，确保内存操作的顺序性，阻止编译器和 CPU 对内存操作进行不必要的重排序。内存屏障可以确保跨越屏障的读写操作不会交叉进行，以此维持程序的内存一致性模型。 在 Java 内存模型（JMM）中，volatile 关键字用于修饰变量时，能够保证该变量的可见性和有序性。关于有序性，volatile 通过内存屏障的插入来实现： 写内存屏障（Store Barrier &#x2F; Write Barrier）： 当线程写入 volatile 变量时，JMM 会在写操作前插入 StoreStore 屏障，确保在这次写操作之前的所有普通写操作都已完成。接着在写操作后插入 StoreLoad 屏障，强制所有后来的读写操作都在此次写操作完成之后执行，这就确保了其他线程能立即看到 volatile 变量的最新值。 读内存屏障（Load Barrier &#x2F; Read Barrier）： 当线程读取 volatile 变量时，JMM 会在读操作前插入 LoadLoad 屏障，确保在此次读操作之前的所有读操作都已完成。而在读操作后插入 LoadStore 屏障，防止在此次读操作之后的写操作被重排序到读操作之前，这样就确保了对 volatile 变量的读取总是能看到之前对同一变量或其他相关变量的写入结果。 通过这种方式，volatile 关键字有效地实现了内存操作的顺序性，从而保证了多线程环境下对 volatile 变量的操作遵循 happens-before 原则，确保了并发编程的正确性。 MESI协议全称为 Modified,Exclusive ，Shared,Invalid，是一种高速缓存一致性协议，为了解决CPU在并发环境下多个CPU缓存不一致而提出的 它定义个高速缓存中数据的四种状态 Modified（M）：表示缓存行已经被修改，但还没有被写回主存储器。在这种状态下，只有一个 CPU 能独占这个修改状态。 Exclusive（E）：表示缓存行与主存储器相同，并且是主存储器的唯一拷贝。这种状态下，只有一个 CPU 能独占这个状态。 Shared（S）：表示此高速缓存行可能存储在计算机的其他高速缓存中，并且与主存储器匹配。在这种状态下，各个 CPU 可以并发的对这个数据进行读取，但都不能进行写操作。 Invalid（I）：表示此缓存行无效或已过期，不能使用。 MESI 协议的主要用途是确保在多个 CPU 共享内存时，各个 CPU 的缓存数据能够保持一致性。当某个 CPU 对共享数据进行修改时，它会将这个数据的状态从 S（共享）或 E（独占）状态转变为 M（修改）状态，并等待适当的时机将这个修改写回主存储器。同时，它会向其他 CPU 广播一个“无效消息”，使得其他 CPU 将自己缓存中对应的数据状态转变为I（无效）状态，从而在下次访问这个数据时能够从主存储器或其他 CPU 的缓存中重新获取正确的数据。","tags":[{"name":"JUC","slug":"JUC","permalink":"https://se-daming.github.io/tags/JUC/"}]},{"title":"MySQL索引高度计算","date":"2024-10-18T02:53:45.000Z","path":"2024/10/18/MySQL索引高度计算/","text":"B+树三层就可存储千万级别数据 以聚集索引为例MySQL 是以页的形式组织数据的 若有为2层根节点存放主键和指针 设主键为bigint，8字节、指针固定6字节 一页是16k&#x2F;（8+6）&#x3D;1000，说明根节点可储存1000个指针 假设一行数据是300字节，则一个数据页16k&#x2F;300&#x3D;50条数据 1000*50&#x3D;5w数据 若为3层同样根节点存储1000个指针指向1000个数据页，每个数据页同样有1000个指针指向第三层的数据页 第三层每一个数据页50条记录 则总数据1000x1000x50&#x3D;5000w","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://se-daming.github.io/tags/MySQL/"}]},{"title":"快手","date":"2024-10-17T15:39:22.000Z","path":"2024/10/17/快手/","text":"1.mysql索引为什么要使用b+树，而不使用b树、AVL为什么不用B树B树的所有节点既存放k也有value，B+树只有叶子节点，非只有k B树的叶子节点是独立的，B+树叶子节点间通过链表连接 B树检索时可能没到叶子节点就结束了，B+树的效率很稳定 范围查找时B树找到下限后进行中序遍历然后找上线，B+树只需遍历链表 B+树层高一般3-4层就可存储千万数据，磁盘io最多也3，4次。 综合：B+树更少的io次数，更高效的查询效率，更适合范围查询 为什么不用AVLAVL每个节点只保存一个数据，磁盘io次数更多 AVL需要旋转来保持平衡，效率低下 2.为什么千万级别的数据b+树只需要三到四层？（具体的计算过程忘记了，这个问题是上个问题自己引出来的，长记性：不要给自己挖坑）3.最左匹配原则：联合索引（a，b，c，d），判断各个条件下走索引的情况：a&#x3D;1 and b &#x3D; 2 and d &#x3D; 3 and c &#x3D;4a&#x3D;1 and b &#x3D; 2 and c &gt; 1（这里说太快了，想都没想就说了，说成c不会走索引，实际应该是c会走，但d不走，这里都会犯错不该） 4.mysql事务特性以及怎么实现的？5.MVCC能保证哪个事务隔离级别（读已提交和可重复读，这里自己提了两种隔离级别read view生成时机的不同，好！）6.MVCC有哪些好处（我觉得最重要的是并发读无需加锁和隔离性吧，还有解决脏读和可重复读这些）7.java基础数据类型（平时没怎么用过byte忘记还有这个了，说应该是七种，实际是八种，我真是癫了）8.int占几个字节（4个字节，算是猜对了）9.讲讲ConcurrentHashMap（jdk1.7和jdk1.8的都说了，对比不同和提升）10.什么情况下链表会转成红黑树，红黑树什么情况下会转为链表（8和6，这里提了一下为什么是8和6，前面看过）11.concurrentHashMap的get方法是否加锁？（没有加锁，提了一下底层用来volatile来修饰value和size，保证可见性）12.那你解释一下volatile这个关键字（讲了讲可见性怎么实现的，说了一下总线嗅探，以及MESI协议）13.那你了解ArrayList和hashMap的扩容机制吗？（ArrayList很久没看了，就记得是扩为1.5倍，HashMap讲的比较详细）14.redis的多路复用有了解过吗？（不会，有了解过但忘记了，半年前看的早忘了）15.redis的分布式锁你的理解（因为我是项目中对redisson分布式锁进行封装，我觉得这种问题问你理解就是想看看你为什么要选择这种分布式锁实现方式，应该要对比redis本身的setnx的缺点，还有为什么选择redisson分布式锁，比redis原生的好在哪，我觉得问理解还可以说分布式锁的意义，总之这种题比较开放）","tags":[{"name":"快手日常","slug":"快手日常","permalink":"https://se-daming.github.io/tags/%E5%BF%AB%E6%89%8B%E6%97%A5%E5%B8%B8/"}]},{"title":"hot100","date":"2024-10-17T15:33:07.000Z","path":"2024/10/17/hot100/","text":"31 下一个排列1将nums = [1,2,7,4,3,1]变成nums = [1,3,1,2,4,7] steps： 先找出最大的索引 k 满足 nums[k] &lt; nums[k+1]，如果不存在，就翻转整个数组；再找出另一个最大索引 l 满足 nums[l] &gt; nums[k]；交换 nums[l] 和 nums[k]；最后翻转 nums[k+1:]。 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public void nextPermutation(int[] nums) &#123; int i=nums.length-2; while(i&gt;=0)&#123; if(nums[i]&lt;nums[i+1])&#123; break; &#125; i--; &#125; if(i&lt;0)&#123; reverse(nums,0); return; &#125; int j=nums.length-1; while(j&gt;=0)&#123; if(nums[j]&gt;nums[i])&#123; break; &#125; j--; &#125; swap(nums,i,j); reverse(nums,i+1); &#125; void swap(int[]nums,int i,int j)&#123; int tmp=nums[i]; nums[i]=nums[j]; nums[j]=tmp; &#125; void reverse(int[]nums,int begin)&#123; int end=nums.length-1; while(begin&lt;end)&#123; swap(nums,begin,end); begin++; end--; &#125; &#125;&#125; 142 环形链表II找出环形链表的起始位置 12345678910111213141516171819202122//快指针和慢指针相遇时，维护一个ptr、它和慢指针同时走，相遇时即为ruopublic ListNode detectCycle(ListNode head) &#123; ListNode slow=head; ListNode fast=head; while(fast!=null)&#123; slow=slow.next; if(fast.next!=null) fast=fast.next.next; else&#123; return null; &#125; if(slow==fast)&#123; ListNode ptr=head; while(ptr!=slow)&#123; ptr=ptr.next; slow=slow.next; &#125; return slow; &#125; &#125; return null; &#125;","tags":[{"name":"算法","slug":"算法","permalink":"https://se-daming.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"基于SPI修改Sharding-JDBC实现Nacos公共配置","date":"2024-10-17T07:28:53.000Z","path":"2024/10/17/基于SPI修改Sharding-JDBC实现Nacos公共配置/","text":"","tags":[]},{"title":"双亲委派机制是什么，如何打破","date":"2024-10-17T06:25:37.000Z","path":"2024/10/17/双亲委派机制是什么，如何打破/","text":"定义双亲委派是类加载时的一种机制，主要涉及到三个加载器：启动类加载器。扩展类加载器，应用程序加载器 按顺序采取自下而上的顺序委派加载，一个类加载时首先去看应用程序加载器是否加载过、加载过则返回，没有则看扩展类是否加载过、没有则看启动类是否加载过，加载过则返回，没有则从上而下加载。启动类看是否是自己加载的范畴，是则加载并返回，不是则扩展类看是否，直到加载成功 当一个类加载器去加载某个类的时候，会自底向上查找是否加载过，如果加载过就直接返回，如果一直到最顶层的类加载器都没有加载，再由顶向下进行加载。 作用1、避免类重复加载 2、保证类加载的安全性，防止核心类库被改变 如何指定特定的加载器获取到类加载器，调用loadClass方法 如何打破双亲委派1、自定义类加载器重写loadClass方法。Tomcat是这样做的 2、线程上下文类加载器，利用上下文类加载器来加载，JDBC、JNDI 3、Osgi框架的类加载器，Osgi是一套新的类加载器机制 Tomcat是如何做的问题：一个tomcat可运行多个web应用，如果两个应用出现了相同的限定类名，如果不打破的话则不能正确加载 （在JVM中，只有相同类加载器+相同类名才视作同一个类） 做法：tomcat为每一个应用都自定义了一个类加载器去加载对应的类，主要通过该重写ClassLoader中的loadClass方法实现。 涉及到的tomcat类加载器的层次如下 每个Web应用会创建一个独立的WebApp类加载器来实现应用之间的隔离；Shared类加载器用于加载应用间共享的类比如Spring，mybatis； 通用类加载器加载可悲web应用和tomcat内部组件共享的类；Catalina用于加载tomcat自身的类 ClassLoader中四个核心方法 1234567891011public Class&lt;?&gt; loadClass(String name)类加载的入口，提供了双亲委派机制。内部会调用findClass 重要protected Class&lt;?&gt; findClass(String name)由类加载器子类实现,获取二进制数据调用defineClass ，比如URLClassLoader会根据文件路径去获取类文件中的二进制数据。重要protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len)做一些类名的校验，然后调用虚拟机底层的方法将字节码信息加载到虚拟机内存中protected final void resolveClass(Class&lt;?&gt; c)执行类生命周期中的连接阶段，默认是false、也就是不连接，不加载静态方法 loadClass：提供双亲委派机制，调用findClass去根据类路径获取二进制数据，","tags":[{"name":"JVM","slug":"JVM","permalink":"https://se-daming.github.io/tags/JVM/"}]},{"title":"百度","date":"2024-10-16T15:42:46.000Z","path":"2024/10/16/百度/","text":"基本数据类型 byte1 short2 int4 long8 char2 float4 double8 boolean 面向对象三大特性 封装：通过 public private default protected来实现，主要为了限制其他类和包使用 继承：一个对象可继承自另一个对象 多态：一个变量可有多种表现形态。主要由继承来实现，比如teacher和stu都继承自person，则person既可以表现成stu也可以。。。 重载重写+多态+虚方法表 重载是一个类中有多个方法方法名相同，但是参数或参数类型、返回值不同 重写是当继承自另一个类是，子类重写父类的方法 多态如何实现？ 虚方法表是多态实现的一个必要条件，它记录了方法的实现类的地址，用来确定引用变量调用子类方法时具体调用的哪个方法 集合的几个接口介绍一下，并说区别 集合包含list、set、queue三个主要接口、除此之外还有map list是可重复的一些数据、set是不可重复的数据、map是键值对类型数据的集合 list下包含arraylist、linkedlist、vector的子类实现 set包含hashset、treeset的实现 map包含hashmap、linkedhashmap、hashtable、treemap的实现 ArrayList和LinkedList的区别 底层实现不同：a是动态数组、l是链表 插入和删除效率不同：a的插入需要移动后面元素，时间复杂度是On，删除时也需要移动元素，On l的插入要先找到对应元素然后o1插入，总的来说也是on，删除同理 l更适合在头部插入元素，O1、而a是On a支持索引查找，O1、l是On 大多数情况下使用啊 HashMap一整套（数据结构，扩容） 8之前是数组+链表；8之后是数组+链表+红黑树； 扩容过程:先创建原来2倍容量的数组，然后将旧数组元素拷贝到新的数组，然后改变引用指向新的数组 JMM Volatile 是Java中的一个关键字，用来解决可见性和防止指令重排 当一个变量如果没被volatile标记、则他被修改时可能会先放到缓冲区，其他变量读到的仍然是修改前的数值。 指令重排：JVM会对class文件的命令在不影响结果的前提下进行重排序 Java的锁类型 包含公平锁、分公平锁；可重入锁，不可重入锁 主要包含sync实现的和R锁 sync对比R：非公平、可公可非；不可重入，可重入；修饰代码块、方法， 修饰代码块；自动释放，手动释放；重量、轻量；基于监视器monitor实现，基于AQS实现；R可带超时的获取尝试 AQS的非公平实现（顺便把AQS一整套说了） 线程创建方式（线程和线程体）继承thread类重写run方法；实现runnable接口的run方法；实现callable接口的call方法；线程池创建； 为什么会有线程安全多线程下对同一资源进行操作，比如对同一变量自增，需要先读取然后修改。这个过程不是原子性的，可能出现读到的相同，修改后出现问题 JVM内存模型分为线程共享和线程私有堆内存、运行时常量池；Java虚拟机栈、本地方法栈、程序计数器；字符串常量池 GC算法标记清除、标记复制、标记整理对需要清除的区域先标记再清除；会导致内存碎片问题 将内存区域分为TO、from两部分、对需要清除的区域先标记再将未标记的部分整理到to区，将from指向to；内存利用率不高 先标记需要清除的，然后将其复制到内存一侧；效率不高 垃圾回收器搭配serial，serial oldparaneel、paraneel newserial CMSg1 三色标记算法 CMS,G1CMS是老年代的回收，G1是新生代和老年代；CMS用标记清除算法，G1用标记整理算法CMS：初始标记、并发标记、最终标记、并发清除G1：初始标记、并发标记、并发标记、并发清除CMS会产生内存碎片和浮动垃圾，严重时退化成serial Springboot启动流程 MySQL索引（ACID，事务隔离，MVCC） MySQL锁用来解决并发下的问题全局锁、表级锁、行级锁表锁、元数据锁、意向锁；行锁、间隙锁、临建锁，建立在索引上的，没索引则升级为表锁 MySQL三大日志redolog、undolog、binlog重做日志，用来做崩溃后的恢复；用来事务回滚和MVCC；数据备份用的 MySQL慢SQL排查 假设慢SQL不是索引的问题，可能是哪些原因表数据量过大、考虑分表；表或查询记录被上锁；MySQL抖了一下，刷新脏页到磁盘时 InnoDB的各种特性事务、外键、行级锁、崩溃后数据恢复、mvcc 说一下项目难点，并说说怎么解决 手撕快排","tags":[{"name":"百度日常","slug":"百度日常","permalink":"https://se-daming.github.io/tags/%E7%99%BE%E5%BA%A6%E6%97%A5%E5%B8%B8/"}]},{"title":"分片上传和断点续传如何实现","date":"2024-10-16T14:25:07.000Z","path":"2024/10/16/分片上传和断点续传如何实现/","text":"j用到的技术：Redis、MD5 分片上传前端将文件分片后携带文件id、各个分片的索引，总分片数。文件MD5 后端将总分片数、已经上传的分片保存到redis。当所有分片上传完成后，后端组装这些分片并计算出MD5值验证是否组装成功。 断点续传继续上传时先查询已经上传的分片、从该分片索引处继续上传","tags":[{"name":"实习","slug":"实习","permalink":"https://se-daming.github.io/tags/%E5%AE%9E%E4%B9%A0/"}]},{"title":"Kafka的架构是怎样的","date":"2024-10-16T10:15:04.000Z","path":"2024/10/16/Kafka的架构是怎样的/","text":"角色生产者、broker、消费者、topic、partition、leader、follower、ISR、zookeeper 基本原理生产者将消息发送到broker的某个topic的某个partition中，消费者从topic中获取数据，使用zk来管理集群状态和配置。这就是基础架构 生产者发送消息主要涉及到两个线程sender和main main线程中生产者调用send方法携带消息经拦截器-分区器-序列化器到达发送缓冲区双端队列RecordAccumulator，当队列中消息量达到batch-size或linger-ms的阈值则sender线程从中拉取消息到broker。过程是拉取消息构建相应的请求到NetworkClient，最多有五个请求，selector负责从中拉取消息发送到broker的分区，等待ack应答，如果acks参数设为0则不等待应答直接返回成功，如果设为1则partition的leader需要应答，如果设为-1则leader和ISR都要应答；当selector收到成功应答后则返回成功，并且清理掉缓冲队列中的信息，否则retries重试发送给分区。 消费者消费消息采用pull拉的方式主动拉取消息，但这样可能导致没数据时陷入循环，一直返回空数据 消费者工作流程 消费过程消费者发送消费请求sendFetches后创建网络请求ConsumerNetworkClient，携带着每批次最小抓取大小Fetch-min-bytes、最大抓取大小Fetch-max-bytes、超时时间Fetch-max-wait-ms调用send方法发送请求给broker，通过回调方法onSuccess返回数据到队列completedFetches中，消费者每一批次最大拉取Max-poll-records条数据后经过反序列化、拦截器后处理数据","tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://se-daming.github.io/tags/Kafka/"}]},{"title":"ConcurrentHashMap解析","date":"2024-10-16T08:49:34.000Z","path":"2024/10/16/ConcurrentHashMap解析/","text":"1 ConcurrentHashMap的底层结构jdk1.7是数组链表 ，类似于HashMap数组 1.8是数组+链表+红黑树，和HashMap类似 ConcurrentHashMap如何保证线程安全jdk1.8之前包含一个Segment数组（默认是16个，创建后不可更改，最大并发是16）、每一个segment元素都是一个HashEntry数组，类似于HashMap，每一个HashEntry元素是链表 Segment继承了Reentrantlock，是一种可重入锁。每一个segment守护者一个HashEntry数组的元素，要更改元素首先要获取对应的segment锁。 也就是说、同一segment的写会阻塞，不同segment可并发执行 （get要加锁吗） jdk1.8取消了segment分段锁，采用volatile（每个node的value和next）+CAS+sync保证并发安全。 增加元素时先判断容器是否为空。为空则CAS来初始化。否则看元素位置处是否为空，为空则CAS赋值。不为空则sync后遍历赋值 数据结构和hashmap类似，由数组链表红黑树组成。只锁定当前链表或红黑树的首节点来保证并发安全。锁的粒度更小 总结：jdk1.8中通过对头节点加锁来确保线程安全。锁的粒度更小，并发量更大。 为什么要用CAS和Sync计算出该位置处为空则hash碰撞的几率低，用较少的自旋完成put；有元素说明发生了哈希碰撞，有大量线程访问或容量不够了则用悲观锁sync get操作为什么不加锁node的value和next是用volatile修饰的，可以保证可加性","tags":[{"name":"集合","slug":"集合","permalink":"https://se-daming.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"HashMap解析","date":"2024-10-16T08:11:52.000Z","path":"2024/10/16/HashMap解析/","text":"h HashMap的底层原理底层结构是数组、链表、红黑树 HashMap的put流程先看table是否为空，为空则初始化，否则则计算元素下标，如果此处没元素则直接put。如果有则看当前元素是否等于插入元素，等于则覆盖。不等于则遍历链表或红黑树来查找相同的元素，找到则覆盖，找不到则插入到红黑树或链表头部。如果是链表则看是否达到阈值8，达到8且hashmap的数组长度大于64则转换成红黑树；至此插入元素完成，然后检查负载因子是否超过阈值0.75，超过则扩容，否则执行结束 扩容过程先创建一个原数组两倍大小的新数组，将原来的引用指向新数组并更新扩容阈值，将旧数组的元素重新计算哈希码并分配到新数组中， HashMap的get流程计算出元素的索引位置，如果当前位置为null则直接返回，否则看当前元素是否等于k，等于直接返回。否则遍历链表或红黑树直到找到相同的元素；如果不存在则返回null； 为什么线程不安全1、两个线程同时put导致数据丢失。两个元素计算得到的索引一样，并且得到的新插入的位置一样，会导致覆盖其他线程的数据 2、jdk1.8前多线程下扩容会导致死循环 为什么容量是2的n次方1、方便计算索引位置。利用与运算计算代替除运算，更快速的计算出位置。但前提是容量是2的n次方 2、扩容后新数组的位置容易确定，分布比较均匀。扩容中只需判断原hash和左移一位，也就是扩大两倍的hash的与运算是0还是1，0则位置不变，1则原位置加上原容量","tags":[{"name":"集合","slug":"集合","permalink":"https://se-daming.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"索引浅析","date":"2024-10-16T07:01:21.000Z","path":"2024/10/16/索引浅析/","text":"1 索引树的高度如何计算","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://se-daming.github.io/tags/MySQL/"}]},{"title":"SQL优化的几种方式","date":"2024-10-16T06:50:38.000Z","path":"2024/10/16/SQL优化的几种方式/","text":"SQL优化插入优化1、使用批量插入 2、手动提交事务 3、主键顺序插入，减少页分裂和页合并的次数 4、大批量数据使用load命令 主键设计尽量降低主键的长度、主键应当是有序自增的、避免使用UUID或其他业务主键，如身份证、业务操作时尽量不更改主键 为什么不用UUID非递增、太长（36字符）导致占用内存大，索引树的高度大，磁盘IO次数多，性能差 查询优化查询时尽量使用索引查询 索引优化尽量使用覆盖索引、多字段排序时要遵循最左前缀法则、 深度分页延迟关联或子查询优化 group byorder by优化MySQL排序有using index和using filesort两种实现方式，要尽量优化成效率高的using index，避免额外排序 也就是说，要对排序字段创建索引并尽量使用覆盖索引；如果不可避免的出现filesort可以适当增加排序缓冲区的大小 更新优化分批更新、限制更新的行数，避免大事务； 在更新的列上有索引，避免行锁升级为表锁 删除优化大批数据删除要分批次删除，避免大事务阻塞；用LIMIT控制行数 delete和truncate区别delete是DML、truncate是DDL delete删除后可回滚、truncate不能 delete对每一行删除会记录日志，速度慢；truncate不记录，直接释放整个数据页，不支持where条件 delete删除后表的结构属性不变；truncate删除后AUTO-INCREMENT重置 explain关键字 type:连接类型。性能：NULL（select 1） &gt; system （表只有一行、系统表）&gt; const（常量查找，主键或唯一索引） &gt; eq_ref（主键或唯一索引的等值匹配） &gt; ref （非唯一索引的查找）&gt; range（根据索引查找匹配行） &gt; index（扫描整个索引而不是全表扫描） &gt; ALL（全表扫描） 除了ALL都表示用到了索引 key：实际用到的索引 possible_keys：可能用到的索引 rows ：MySQL估计需要读取的行数 extra 额外信息：using index（MySQL仅通过索引来满足查询而无需读取实际数据行）、using where、using filesort（对结果排序时如果索引不能满足则读取实际数据行然后在内存中排序）、using temporary","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://se-daming.github.io/tags/MySQL/"}]},{"title":"设计一个秒杀系统","date":"2024-10-15T04:10:16.000Z","path":"2024/10/15/设计一个秒杀系统/","text":"设计将库存信心保存到redis、利用其抗高并发的特性进行库存的查询和扣减 超卖当买多卖少时，超卖最可能发生 如果允许同一个用户购买多个，则使用redis的decr命令即可、但是会导致库存数为负数。需要在decr返回值小于0时再incr回来以确保超时订单库存回滚正常 如果只允许一个用户购买一个，也就是超买问题 超买判断用户是否购买、判断库存是否充足、扣减库存 其中第二三步redis可解决 判断是否购买放到set集合里面。 分布式环境下就不能放到set、考虑放到redis 原子性MySQL存 ：库存扣了、没有订单 Redis存：数据一致性。redis做了资格判断和扣减库存，没来得及发送到mq操作数据库就挂了","tags":[{"name":"项目","slug":"项目","permalink":"https://se-daming.github.io/tags/%E9%A1%B9%E7%9B%AE/"}]},{"title":"redis单线程，出现阻塞了怎么办","date":"2024-10-15T02:57:26.000Z","path":"2024/10/15/redis单线程，出现阻塞了怎么办/","text":"","tags":[]},{"title":"redis线程模型","date":"2024-10-15T02:20:11.000Z","path":"2024/10/15/redis线程模型/","text":"i BIO、NIO、IO多路复用、信号驱动IO、异步IOBIONIOIO多路复用selectpollepoll","tags":[{"name":"Redis","slug":"Redis","permalink":"https://se-daming.github.io/tags/Redis/"}]},{"title":"https连接是如何创建的","date":"2024-10-15T00:49:33.000Z","path":"2024/10/15/https连接是如何创建的/","text":"https握手过程hhttps在建立TCP连接后会进行TLS连接，主要涉及四次握手 客户端发送Client hello请求给服务端，携带支持的TLS版本、一个随机数、（加密）密码套件列表 服务端响应Server hello，携带确认的TLS版本，一个随机数，server端安全证书，确认的密码套件 客户端收到响应后先确认CA证书的真实性如果没问题则从证书中取出公钥然后用它加密报文发送：一个随机数、加密通信算法改变通知（表示以后的通信都用会话密钥加密）、客户端握手阶段结束。客户端根据这三个随机数计算出会话密钥 服务端收到第三个随机数后通过协商的加密算法计算出会话密钥，向客户端发送：加密算法改变通知、服务端握手结束 HTTP VS https端口不同、80 443 https在建立TCP连接轴要经过TLS&#x2F;SSL握手过程再建立http连接 https是加密传输、http是明文传输 https要额外申请CA证书","tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://se-daming.github.io/tags/HTTP/"}]},{"title":"http报文格式是怎样的","date":"2024-10-14T09:42:31.000Z","path":"2024/10/14/http报文格式是怎样的/","text":"请求报文请求行、首部行、空行、请求实体 12345GET /example.com/index.html HTTP1.1Host: www.example.comConnection:closeUser-Agent: Mozilla/5.0Accept-language:CN 响应报文状态行、首部行、空行、相应实体 12345HTTP1.1 200 OKConnection:closeLast-Modified: Tue,18 Aug 2020Cotent-Length:6123Cotent-Type:text/html Connection字段是什么发送完该消息后关闭TCP连接","tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://se-daming.github.io/tags/HTTP/"}]},{"title":"http2相对于http1.1到底有哪些改变","date":"2024-10-14T07:58:01.000Z","path":"2024/10/14/http2相对于http1-1到底有哪些改变/","text":"HTTP2的目的减少感知时延，提高响应速度 手段经单一TCP连接使请求和响应多路复用 效果提供报文优先次序、服务器主动推送（如果没发送任何请求，只是建立了TCP连接可以主动推送吗）、首部字段压缩 HTTP1.1存在的问题 HOL 队首阻塞HTTP1使用持续TCP连接，允许经单一TCP连接请求相应一个web页面。这样每个web页面平等共享带宽，导致HOL 假如一个web页面有一个大视频对象和视频下面很多小对象，使用同一TCP连接请求、该大视频会花费很长时间，小对象被延迟了。这就是HOL队首阻塞 HTTP1.1解决HOL打开多个并行TCP连接、web页面的多个对象并行发送给浏览器 HTTP2解决HOL将每个报文分成二进制小帧，并且在同个TCP连接上交错发请求和响应报文，之后在接收端将其组装，这也是HTTP2最为重要的改进 成帧过程主要通过HTTP2的成帧子层完成.当服务器要发送HTTP响应时，该响应首先划分成帧，响应的首部字段成为一帧，报文体成为一帧。通过成帧子层该响应的帧与其他响应的帧交错经过单一TCP连接发送。当帧到达客户端时先在成帧子层组装成初始的响应报文，然后由浏览器处理。类似地、HTTP请求也划分成帧交错发送 服务器推送HTTP1.1中，服务器只是被动响应，浏览器请求什么就响应什么。HTTP2中服务器可以主动推送，即为一个请求发送多个响应，从而消除额外等待时延 首部压缩HTTP2会压缩请求头。如果发送的多个请求的请求头一样或类似。协议会消除重复的部分。这由HPACK算法实现：客户端和服务端同时维护一张头信息表，所有字段会存入这个表，生成一个索引号，以后就只发送索引号不发送字段，从而提高速度 总结HTTP2相对于HTTP1主要提高了响应性能，做了头部压缩、二进制成帧、请求响应多路复用、服务器主动推送","tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://se-daming.github.io/tags/HTTP/"}]},{"title":"first","date":"2024-10-14T03:50:05.000Z","path":"2024/10/14/first/","text":"这是我的第一篇博客，加油！haha","tags":[]}]